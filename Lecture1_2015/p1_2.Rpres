```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```

Digital Data Collection - getting started
========================================================
width: 1200
author: Rolf Fredheim and Yulia Shenderovich
date: University of Cambridge
font-family: 'Rockwell'
css:style.css

17/02/2014

Logging on
========================================================
type: s1

Before you sit down:
- Do you have your MCS password?
- Do you have your Raven password?
  - If you answered **'no'** to either then go to the University Computing Services (just outside the door) NOW!
- Are you registered? If not, see me!

Download these slides 
========================================================
type:sq2

Follow link from course description on the SSRMC pages or go directly to 
http://fredheir.github.io/WebScraping/

Download the R file to your computer

Optionally download the slides

And again, optionally open the html slides in your browser



Install the following packages:
===============
knitr
ggplot2
lubridate
plyr
jsonlite
stringr

press **preview** to view the slides in RStudio


Who is this course for
===============
<s>Computer scientists</s>

Anyone with some minimal background in coding and good computer literacy


By the end of the course you will have
==============
Created a system to extract text and numbers from a large number of web pages

Learnt to harvest links

Worked with an API to gather data, e.g. from YouTube

Convert messy data into tabular data


What will we need?
==============
A windows Computer

A modern browser - Chrome or Firefox

<s>~~An up to date version of Rstudio~~</s>


Getting help
============
- ?[functionName]
- StackOverflow
- Ask each other. 


Outline
========================================================
type:section

**Theory**

Practice


What is 'Web Scraping'?
========================================================
From [Wikipedia](http://en.wikipedia.org/wiki/Web_scraping)
> Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites.


When might this be useful? (your examples)

- 
- 
- 
- 

Imposing structure on data
=========
Again, from [Wikipedia](http://en.wikipedia.org/wiki/Web_scraping)
> ... Web scraping focuses on the **transformation of unstructured data** on the web, typically in HTML format, into structured data that can be stored and analyzed in **a central local database or spreadsheet**. 



What will we learn? 
====================
1) working with text in R

2) Connecting R to the outside world

3) Downloading from within R



Example
=======
Approximate number of web pages

<img src="https://github.com/fredheir/WebScraping/blob/master/Lecture1/i2.jpg?raw=true" alt="Drawing" />

Tabulate this data
======

```{r}
require (ggplot2)
clubs <- c("Tottenham","Arsenal","Liverpool",
           "Everton","ManU","ManC","Chelsea")
nPages <- c(67,113,54,16,108,93,64)
df <- data.frame(clubs,nPages)
df
```

Visualise it
=======
```{r fig.width=30}
ggplot(df,aes(clubs,nPages,fill=clubs))+
  geom_bar(stat="identity")+
  coord_flip()+theme_bw(base_size=70)
```

Health and Safety
=====================
<p align="center"><img src="http://static2.wikia.nocookie.net/__cb20130318135906/deadfrontier/images/c/cb/Warning.png" alt="Drawing"style="width: 30%;"/></p>

Programming with Humanists: Reflections on Raising an Army of Hacker-Scholars in the Digital Humanities
http://openbookpublishers.com/htmlreader/DHP/chap09.html#ch09


Why might the Google example not be a good one?
=====================

Bandwidth
=================
<p align="center"><img src="http://www.cisco.com/web/about/ac123/ac147/images/ipj/ipj_7-4/dos_figure_4.gif" alt="Drawing" /></p>
***
> the agent machines (slave zombies) begin to send a large volume of packets to the victim, flooding its system with useless load and exhausting its resources.

source: cisco.com

We will not: 
- run parallel processes

we will:
- test code on minimal data

Practice
==============
type:section
- **String manipulation**
- Loops
- Scraping

The JSON data
==================

http://stats.grok.se/json/en/201401/web_scraping

{"daily_views": {"2013-01-12": 542, "2013-01-13": 593, "2013-01-10": 941, "2013-01-11": 798, "2013-01-16": 1119, "2013-01-17": 1124, "2013-01-14": 908, "2013-01-15": 1040, "2013-01-30": 1367, "2013-01-18": 1027, "2013-01-19": 743, "2013-01-31": 1151, "2013-01-29": 1210, "2013-01-28": 1130, "2013-01-23": 1275, "2013-01-22": 1131, "2013-01-21": 1008, "2013-01-20": 707, "2013-01-27": 789, "2013-01-26": 747, "2013-01-25": 1073, "2013-01-24": 1204, "2013-01-01": 379, "2013-01-03": 851, "2013-01-02": 807, "2013-01-05": 511, "2013-01-04": 818, "2013-01-07": 745, "2013-01-06": 469, "2013-01-09": 946, "2013-01-08": 912}, "project": "en", "month": "201301", "rank": -1, "title": "web_scraping"}




String manipulation in R
==============
type:sq2

Top string manipulation functions:
<small>
- tolower (also  toupper, capitalize)
- grep
- gsub
- str_split (library: stringr)
-substring
- paste and paste0
- nchar
- str_trim (library: stringr)
</small>

Reading: 
<small>
- http://en.wikibooks.org/wiki/R_Programming/Text_Processing
- http://chemicalstatistician.wordpress.com/2014/02/27/useful-functions-in-r-for-manipulating-text-data/
- http://gastonsanchez.com/blog/resources/how-to/2013/09/22/Handling-and-Processing-Strings-in-R.html
</small>


Changing the case
================
incremental:true
We can apply them to individual strings, or to vectors:
```{r}
tolower('ROLF')
states = rownames(USArrests)
tolower(states[0:4])
toupper(states[0:4])
```

Number of characters
================
incremental:true
We can also use this to make selections:
```{r}
nchar(states)
states[nchar(states)==5]

```


Cutting strings
============
We can use fixed positions, e.g. to get first character
`r substring('mytextishere',1,1)`

or to get a fixed part of the string:
`r substring('mytextishere',3,6)`

Can you see how this function works? If not use ?substring


str_split
==============
incremental:true
type:sq

- Manipulating URLs
- Editing time stamps, etc

- syntax: str_split(inputString,pattern)
returns a list
```{r}
require(stringr)
link="http://stats.grok.se/json/en/201401/web_scraping"
str_split(link,'/')
unlist(str_split(link,"/"))
```

Cleaning data
============
type:sq1
incremental:true

- nchar
- tolower (also  toupper)
- str_trim (library: stringr)
```{r}
annoyingString <- "\n    something HERE  \t\t\t"
```
***
```{r}
nchar(annoyingString)
str_trim(annoyingString)
tolower(str_trim(annoyingString))
nchar(str_trim(annoyingString))
```


Structured practice
===========
type:alert
Remember how to read in files using R? Load in some text from the web:
<small>
```{r}
require(RCurl)

download.file('https://raw.githubusercontent.com/fredheir/WebScraping/gh-pages/Lecture1_2015/text.txt',destfile='tmp.txt',method='curl')
text=readLines('tmp.txt')

```
- What is this? Explore the file. 
- How many lines does the file have?
- print only the seventh line. Use **str_split()** to break it up into individual words
- How many words are there? use **length()** to count the number of words. 
- Are any words used more than once? Use table to find out!
- Can you sort the results? 
- What are the 10 most common words?
- use nchar to find the length of the ten most common words? Tip: use **names()**
- What about for the whole text?
</small>

Walkthrough
=======

page deliberately left blank

What do they do - grep
=====================
type:sq1
incremental:true
Grep allows regular expressions in R

E.g. 
```{r}
grep("Ohio",states)

grep("y",states)

#To make a selection
states[grep("y",states)]

```

Grep 2
============
type:sq

useful options: 
- invert=TRUE : get all non-matches
- ignore.case=TRUE : what it says on the box
- value = TRUE : return values rather than positions

Structured practice2
===========
type:alert
Use Grep to find all the statements including the words:
- 'London'
- 'conspiracy'
- 'amendment'

Each of the statements in our parliamentary debate begin with a paragraph sign(ยง)
- Use grep to select only these lines!
- How many separate statements are there? 



Walkthrough2
=======

page deliberately left blank

Regex
========
type:sq1
incremental:true

- ?regex 
- http://www.rexegg.com/regex-quickstart.html

Can match beginning or end of word, e.g.:
```{r eval =F}
stalinwords=c("stalin","stalingrad","Stalinism","destalinisation")
grep("stalin",stalinwords,value=T)

#Capitalisation
grep("stalin",stalinwords,value=T)
grep("[Ss]talin",stalinwords,value=T)

#Wildcards
grep("s*grad",stalinwords,value=T)

#beginning and end of word
grep('\\<d',stalinwords,value=T)
grep('d\\>',stalinwords,value=T)
```

Before running these on your computer, can you figure out what they will do?

Structured practice 3
===========
type:alert
Use grep to check whether you missed some hits for above due to capitalisation (London, conspiracy, amendment)

Use the caret(^ ) character to match the start of a line. How many lines start with the word 'Amendment'?

Use the dollar($) sign to match the end of a line. How many lines end with a question mark?


Walkthrough3
=======

page deliberately left blank

What do they do: gsub
=====================
incremental:true
```{r}
author <- "By Rolf Fredheim"
gsub("By ","",author)
gsub("Rolf Fredheim","Tom",author)
```

Gsub can also use regex




Outline
========================================================
type:section

Theory

**Practice**



Questions
===================
type:section
1) how do we read the data from this page
http://stats.grok.se/json/en/201401/web_scraping

2) how do we generate a list of links, say for the whole of 2013?

Practice
==============
type:section

- String manipulation
- **Scraping**
- Loops


The URL
=============

http://stats.grok.se/

http://stats.grok.se/en/201401/web_scraping

- en
- 201401
- web_scraping 

en.wikipedia.org/wiki/Web_scraping

Changes by hand
=====

http://stats.grok.se/en/201301/web_scraping
http://stats.grok.se/en/201402/web_scraping
http://stats.grok.se/en/201401/data_scraping


'this page is in json format'



Paste
==================
incremental:true
Check out ?paste if you are unsure about this 

Bonus: check out ?paste0

```{r}
var=123
paste("url",var,sep="")
paste("url",var,sep=" ")
```

Paste2
==================
incremental:true
```{r}
var=123
paste("url",rep(var,3),sep="_")
```

Paste3
======
Can you figure out what these will print?
```{r eval=F}
paste("url",1:3,var,sep="_")
var=c(123,421)
paste(var,collapse="_")
```

With a URL
===========================
type:sq
incremental:true
```{r}
var=201401
paste("http://stats.grok.se/json/en/",var,"/web_scraping")
paste("http://stats.grok.se/json/en/",var,"/web_scraping",sep="")
```

Task using 'paste'
==============
type:alert
<small>
- a="test"
- b="scrape"
- c=94

merge variables a,b,c into a string, separated by an underscore ("_")
> "test_scrape_94"

merge variables a,b,c into a string without any separating character
> "testscrape94"

print the letter 'a' followed by the numbers 1:10, without a separating character 
> "a1"  "a2"  "a3"  "a4"  "a5"  "a6"  "a7"  "a8"  "a9"  "a10"
</small>

Walkthrough
===========

page deliberately left blank

Testing a URL is correct in R
==============
type:sq1

Run this in your terminal:

```{r eval=F}
var=201401
url=paste("http://stats.grok.se/json/en/",var,"/web_scraping",sep="")
url
browseURL(url)
```

Fetching data
==================
type:sq1
```{r}
var=201401
url=paste("http://stats.grok.se/json/en/",var,"/web_scraping",sep="")
raw.data <- readLines(url, warn="F") 
raw.data
```

Fetching data2
==================
```{r}
require(jsonlite)
rd  <- fromJSON(raw.data)
rd
```

Fetching data3
==================
```{r}
rd.views <- unlist(rd$daily_views)
rd.views
```

Fetching data4
==================
```{r}
rd.views <- unlist(rd.views)
df <- as.data.frame(rd.views)
df
```

Put it together
===================
type:sq1
```{r eval=F}
var=201403

url=paste("http://stats.grok.se/json/en/",var,"/web_scraping",sep="")
rd <- fromJSON(readLines(url, warn="F"))
rd.views <- rd$daily_views 
df <- as.data.frame(unlist(rd.views))
```

Can we turn this into a function? 
=========================
type:alert
Select the four lines in the previous slide, go to 'code' in RStudio, and click function

This will allow you to make a function, taking one input, 'var'

In future you can then run this as follows:

```{r eval=F} 
df=myfunction(var) 
```

Plot it
=================
type:sq
```{r}
require(ggplot2)
require(lubridate)
df$date <-  as.Date(rownames(df))
colnames(df) <- c("views","date")
ggplot(df,aes(date,views))+
  geom_line()+
  geom_smooth()+
  theme_bw(base_size=20)
```

Tasks
====================
type:alert
Plot Wikipedia page views for February 2015. How do these compare with the numbers for 2014?


What about some other event? Modify the code below to checkout stats for something else?  
paste("http://stats.grok.se/json/en/",var,"/web_scraping",sep="")

Now try changing the language of the page ('en' above). How about Russian, or German? 

Moving on
=========================
Now we will learn about loops


Practice
==============
type:section
- String manipulation
- Scraping
- **Loops**


Idea of a loop
========
type:sq1
Purpose is to reuse code by using one or more variables. Consider:
```{r}
name='Rolf Fredheim'
name='Yulia Shenderovich'
name='David Cameron'
firstsecond=(str_split(name, ' ')[[1]])
ndiff=nchar(firstsecond[2])-nchar(firstsecond[1])
print (paste0(name,"'s surname is ",ndiff," characters longer than their firstname"))


```

Simple loops
=============
<small> 
- Curly brackets {} include the code to be executed
- Normal brackets () contain a list of variables</small>

```{r}
for (number in 1:5){
	print (number)
}
```

Looping over functions
========================
type:sq2
incremental:true

```{r}
states_first=head(states)
for (state in states_first){
	print (
		tolower(state)
	)
}

for (state in states_first){
  print (
		substring(state,1,4)
	)
}
```


Urls again
==========
type:sq1

stats.grok.se/json/en/**201401**/web_scraping
```{r}
for (month in 1:12){
	print(paste(2014,month,sep=""))
}
```

Not quite right
================
type:sq
left:20
We need the variable 'month' to have two digits:

201401
***
```{r}
	for (month in 1:9){
		print(paste(2012,0,month,sep=""))
	}

	for (month in 10:12){
		print(paste(2012,month,sep=""))
	}
```

Store the data
=========
type:sq
left:60
```{r}
dates=NULL
	for (month in 1:9){
		date=(paste(2012,0,month,sep=""))
		dates=c(dates,date)
	}

	for (month in 10:12){
		date=(paste(2012,month,sep=""))
		dates=c(dates,date)
	}
print (as.numeric(dates))
```
***
here we concatenated the values:
```{r}
dates <- c(c(201201,201202),201203)
print (dates)

```
!! To do this with a **data.frame**, use **rbind()**

Putting it together
============================
type:sq
```{r}
  for (month in 1:9){
		print(paste("http://stats.grok.se/json/en/2013",0,month,"/web_scraping",sep=""))
	}

	for (month in 10:12){
		print(paste("http://stats.grok.se/json/en/2013",month,"/web_scraping",sep=""))
	}
```


Tasks about Loops
==================
type:alert
- Write a loop that prints every number between 1 and 1000
- Write a loop that adds up all the numbers between 1 and 1000
- Write a function that takes an input number and returns this number divided by two
- Write a function that returns the value 99 no matter what the input
- Write a function that takes two variables, and returns the sum of these variables

If you want to take this further....
=========================
type: section
- Can you make an application which takes a Wikipedia page (e.g. Web_scraping) and returns a plot for the month 201312
- Can you extend this application to plot data for the entire year 2013 (that is for pages 201301:201312)
- Can you expand this further by going across multiple years (201212:201301)
- Can you write the application so that it takes a custom data range?
- If you have time, keep expanding functionality: multiple pages, multiple languages. you could also make it interactive using [Shiny](http://www.rstudio.com/shiny/)


Reading
=============

http://www.bbc.co.uk/news/technology-23988890
http://blog.hartleybrody.com/web-scraping/
http://openbookpublishers.com/htmlreader/DHP/chap09.html#ch09
http://www.essex.ac.uk/ldev/documents/going_digital/scraping_book.pdf
https://software.rc.fas.harvard.edu/training/scraping2/latest/index.psp#(1) 


